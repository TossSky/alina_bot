# app/llm_client.py
from __future__ import annotations
import asyncio
import os
from typing import List, Dict, Optional
import random
import re

import httpx
import google.generativeai as genai
from google.generativeai import types as genai_types

from .config import settings
from .prompts import REFUSAL_STYLE, HUMANITY_HINTS

# –ë–∞–∑–æ–≤—ã–µ –¥–µ—Ñ–æ–ª—Ç—ã
DEFAULT_TEMPERATURE = 0.92  # –ü–æ–≤—ã—Å–∏–º –¥–ª—è –±–æ–ª–µ–µ –∂–∏–≤—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
DEFAULT_MAX_TOKENS = 400    # –£–º–µ–Ω—å—à–∏–º –¥–ª—è –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤

# –ü—Ä–æ—Ñ–∏–ª–∏ –ø–æ–¥ –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–∞
VERBOSITY_PROFILES = {
    "short":  {"max_tokens": 150, "temperature": 0.95},  # –û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ, –∂–∏–≤—ã–µ
    "normal": {"max_tokens": 300, "temperature": 0.92},  # –£–º–µ—Ä–µ–Ω–Ω—ã–µ
    "long":   {"max_tokens": 450, "temperature": 0.90},  # –ß—É—Ç—å –¥–ª–∏–Ω–Ω–µ–µ
}

# –§—Ä–∞–∑—ã –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —á–µ–ª–æ–≤–µ—á–Ω–æ—Å—Ç–∏
HUMAN_TOUCHES = {
    "thinking": ["—Ö–º", "–Ω—É", "—ç–º", "–≤–æ—Ç", "–∫—Å—Ç–∞—Ç–∏"],
    "uncertainty": ["–Ω–∞–≤–µ—Ä–Ω–æ–µ", "–º–æ–∂–µ—Ç", "–≤—Ä–æ–¥–µ", "–∫–∞–∂–µ—Ç—Å—è", "–ø–æ—Ö–æ–¥—É"],
    "endings": [")", "...", "üåø", "üíõ", ""],
}

def _pick_profile(verbosity: Optional[str]) -> Dict[str, float]:
    if not verbosity:
        return {"max_tokens": DEFAULT_MAX_TOKENS, "temperature": DEFAULT_TEMPERATURE}
    verbosity = verbosity.lower().strip()
    return VERBOSITY_PROFILES.get(verbosity, {"max_tokens": DEFAULT_MAX_TOKENS, "temperature": DEFAULT_TEMPERATURE})

def _humanize_text(text: str) -> str:
    """–î–æ–±–∞–≤–ª—è–µ—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ —à—Ç—Ä–∏—Ö–∏ –∫ —Ç–µ–∫—Å—Ç—É"""
    if not text:
        return text
    
    # –ò–Ω–æ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–µ–º –≤–≤–æ–¥–Ω—ã–µ —Å–ª–æ–≤–∞
    if random.random() < 0.3:
        intro = random.choice(HUMAN_TOUCHES["thinking"])
        text = f"{intro}, {text[0].lower()}{text[1:]}"
    
    # –ò–Ω–æ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–µ–º –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
    if random.random() < 0.2:
        uncertainty = random.choice(HUMAN_TOUCHES["uncertainty"])
        # –í—Å—Ç–∞–≤–ª—è–µ–º –≤ —Å–ª—É—á–∞–π–Ω–æ–µ –º–µ—Å—Ç–æ
        words = text.split()
        if len(words) > 3:
            pos = random.randint(1, len(words) - 2)
            words.insert(pos, uncertainty)
            text = " ".join(words)
    
    # –ò–Ω–æ–≥–¥–∞ –¥–µ–ª–∞–µ–º "–æ–ø–µ—á–∞—Ç–∫—É" (–æ—á–µ–Ω—å —Ä–µ–¥–∫–æ)
    if random.random() < 0.05:
        text = _add_typo(text)
    
    return text

def _add_typo(text: str) -> str:
    """–î–æ–±–∞–≤–ª—è–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—É—é –æ–ø–µ—á–∞—Ç–∫—É"""
    typos = [
        ("—á—Ç–æ", "—á—Ç"),
        ("—Å–µ–π—á–∞—Å", "—Å—á–∞—Å"),
        ("–º–æ–∂–µ—Ç", "–º–∂–µ—Ç"),
        ("–ø—Ä–∏–≤–µ—Ç", "–ø—Ä–≤–µ—Ç"),
        ("—Å–ø–∞—Å–∏–±–æ", "—Å–ø—Å–∏–±–æ"),
    ]
    for correct, typo in typos:
        if correct in text.lower():
            # –ó–∞–º–µ–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ
            text = text.replace(correct, typo, 1)
            break
    return text

def _postprocess(text: str) -> str:
    """–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞"""
    if not text:
        return text

    # –£–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —è–≤–Ω—ã–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã
    t = text.strip()
    for prefix in ("–ê–ª–∏–Ω–∞:", "–ê–ª–∏–Ω–∞ ‚Äî", "–ê–ª–∏–Ω–∞ -"):
        if t.startswith(prefix):
            t = t[len(prefix):].strip()
            break

    # –î–æ–±–∞–≤–ª—è–µ–º —á–µ–ª–æ–≤–µ—á–Ω–æ—Å—Ç–∏
    t = _humanize_text(t)

    # –ó–∞–º–µ–Ω—è–µ–º Markdown, —á—Ç–æ–±—ã Telegram –µ–≥–æ –ø–æ–Ω—è–ª
    t = re.sub(r'\*\*(.*?)\*\*', r'*\1*', t)

    return t.strip()

class LLMClient:
    """
    –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –∫ LLM-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º.
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö, –∂–∏–≤—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤.
    """

    def __init__(self):
        self.provider = (os.getenv("LLM_PROVIDER") or getattr(settings, "llm_provider", "deepseek")).lower()

        # DeepSeek
        self.deepseek_api_key = os.getenv("DEEPSEEK_API_KEY") or getattr(settings, "deepseek_api_key", None)
        self.deepseek_model = getattr(settings, "deepseek_model", "deepseek-chat")

        # Gemini
        self.gemini_api_key = os.getenv("GEMINI_API_KEY") or getattr(settings, "gemini_api_key", None)
        self.gemini_model = getattr(settings, "gemini_model", "gemini-1.5-flash")
        if self.provider == "gemini" and self.gemini_api_key:
            genai.configure(api_key=self.gemini_api_key)

        # OpenRouter
        self.openrouter_api_key = os.getenv("OPENROUTER_API_KEY") or getattr(settings, "openrouter_api_key", None)
        self.openrouter_model = getattr(settings, "openrouter_model", "openrouter/auto")

        # Ollama
        self.ollama_host = getattr(settings, "ollama_host", "http://127.0.0.1:11434")
        self.ollama_model = getattr(settings, "ollama_model", "qwen2.5")

        self._client: Optional[httpx.AsyncClient] = None

    async def _get_client(self) -> httpx.AsyncClient:
        if self._client is None:
            self._client = httpx.AsyncClient(timeout=httpx.Timeout(15.0, connect=5.0))
        return self._client

    async def chat(
        self,
        messages: List[Dict[str, str]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        *,
        verbosity: Optional[str] = None,
        safety: bool = False,
    ) -> str:
        prof = _pick_profile(verbosity)
        temperature = float(temperature if temperature is not None else prof["temperature"])
        max_tokens = int(max_tokens if max_tokens is not None else prof["max_tokens"])

        if random.random() < 0.3:
            hint = random.choice(list(HUMANITY_HINTS.values()))
            messages = messages + [{"role": "system", "content": hint}]

        if safety:
            messages = [{"role": "system", "content": REFUSAL_STYLE}] + messages

        prov = self.provider
        if prov == "deepseek":
            return await self._chat_deepseek(messages, temperature, max_tokens)
        elif prov == "gemini":
            return await self._chat_gemini(messages, temperature, max_tokens)
        elif prov == "openrouter":
            return await self._chat_openrouter(messages, temperature, max_tokens)
        elif prov == "ollama":
            return await self._chat_ollama(messages, temperature, max_tokens)
        else:
            return await self._chat_deepseek(messages, temperature, max_tokens)

    async def _chat_gemini(
        self,
        messages: List[Dict[str, str]],
        temperature: float,
        max_tokens: int,
    ) -> str:
        if not self.gemini_api_key:
            raise RuntimeError("GEMINI_API_KEY –Ω–µ –∑–∞–¥–∞–Ω")

        system_instructions = [msg["content"] for msg in messages if msg["role"] == "system"]
        
        chat_history = []
        for msg in messages:
            if msg["role"] == "system":
                continue
            # Gemini API –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–æ–ª—å 'model' –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
            role = "model" if msg["role"] == "assistant" else "user"
            chat_history.append({'role': role, 'parts': [msg['content']]})


        model = genai.GenerativeModel(
            model_name=self.gemini_model,
            system_instruction="\n".join(system_instructions),
            generation_config=genai.types.GenerationConfig(
                max_output_tokens=max_tokens,
                temperature=temperature,
                # –û—Ç–∫–ª—é—á–∞–µ–º "thinking" –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤, –∫–∞–∫ –≤ –Ω–æ–≤–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
                thinking_config=genai_types.ThinkingConfig(thinking_budget=0)
            )
        )

        try:
            resp = await model.generate_content_async(chat_history)
            txt = resp.text
            return _postprocess(txt)
        except Exception as e:
            print(f"Gemini API error: {e}")
            return "–û–π, —á—Ç–æ-—Ç–æ –≥–æ–ª–æ–≤–∞ —Ä–∞–∑–±–æ–ª–µ–ª–∞—Å—å... –¥–∞–≤–∞–π —á—É—Ç—å –ø–æ–∑–∂–µ?"


    async def _chat_deepseek(
        self,
        messages: List[Dict[str, str]],
        temperature: float,
        max_tokens: int,
    ) -> str:
        if not self.deepseek_api_key:
            raise RuntimeError("DEEPSEEK_API_KEY –Ω–µ –∑–∞–¥–∞–Ω")

        url = "https://api.deepseek.com/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.deepseek_api_key}",
            "Content-Type": "application/json",
        }
        payload = {
            "model": self.deepseek_model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "top_p": 0.95,
            "frequency_penalty": 0.3,
        }

        client = await self._get_client()
        try:
            resp = await client.post(url, headers=headers, json=payload)
            resp.raise_for_status()
            data = resp.json()
            txt = data["choices"][0]["message"]["content"]
            return _postprocess(txt)
        except Exception as e:
            return "–æ–π, —á—Ç–æ-—Ç–æ —Å–≤—è–∑—å –±–∞—Ä–∞—Ö–ª–∏—Ç... –ø–æ–ø—Ä–æ–±—É–π –µ—â—ë —Ä–∞–∑?"

    async def aclose(self):
        if self._client is not None:
            await self._client.aclose()
            self._client = None