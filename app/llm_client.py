# app/llm_client.py
from __future__ import annotations
import sys
import traceback
from typing import List, Dict, Optional
import re

import httpx
from openai import AsyncOpenAI
from openai import AuthenticationError, PermissionDeniedError, APITimeoutError

from .config import settings
from .prompts import REFUSAL_STYLE

# –ë–∞–∑–æ–≤—ã–µ –¥–µ—Ñ–æ–ª—Ç—ã
DEFAULT_TEMPERATURE = 0.92
DEFAULT_max_completion_tokens = 500
MAX_RESPONSE_LENGTH = 800
SHORT_RESPONSE_LENGTH = 300

def _format_lists(text: str) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ —Å –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏ —Å—Ç—Ä–æ–∫"""
    if not text:
        return text
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ø–∏—Å–∫–æ–≤
    patterns = [
        (r'(\d+)\.\s+([^.!?]+?)(?=\s*\d+\.|$)', r'\1. \2'),
        (r'(\d+)\)\s+([^.!?]+?)(?=\s*\d+\)|$)', r'\1) \2'),
    ]
    
    for pattern, replacement in patterns:
        matches = list(re.finditer(pattern, text))
        if matches:
            parts = []
            last_end = 0
            
            for match in matches:
                if match.start() > last_end:
                    parts.append(text[last_end:match.start()].rstrip())
                
                item = match.group(1) + '. ' + match.group(2).strip()
                parts.append('\n' + item)
                last_end = match.end()
            
            if last_end < len(text):
                remaining = text[last_end:].lstrip()
                if remaining:
                    parts.append('\n' + remaining)
            
            text = ''.join(parts).strip()
            break
    
    text = re.sub(r'(?:^|\s)[-‚Ä¢]\s+', '\n‚Ä¢ ', text)
    return text

def _postprocess(text: str) -> str:
    """–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞"""
    if not text:
        return text

    t = text.strip()
    for prefix in ("–ê–ª–∏–Ω–∞:", "–ê–ª–∏–Ω–∞ ‚Äî", "–ê–ª–∏–Ω–∞ -"):
        if t.startswith(prefix):
            t = t[len(prefix):].strip()
            break

    t = _format_lists(t)
    t = re.sub(r'\*\*(.*?)\*\*', r'*\1*', t)
    t = re.sub(r'\n{3,}', '\n\n', t)
    return t.strip()

class LLMClient:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å OpenAI API"""

    def __init__(self):
        self.api_key = settings.openai_api_key
        self.model = settings.openai_model
        self.use_proxy = settings.openai_use_proxy
        self.proxy_address = settings.openai_proxy_address

        if not self.api_key:
            raise RuntimeError("OPENAI_API_KEY –Ω–µ –∑–∞–¥–∞–Ω –≤ .env —Ñ–∞–π–ª–µ")

    async def _create_http_client(self) -> httpx.AsyncClient:
        """–°–æ–∑–¥–∞–µ—Ç HTTP –∫–ª–∏–µ–Ω—Ç —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏"""
        if self.use_proxy and self.proxy_address:
            print(f"[LLM] –°–æ–∑–¥–∞–µ–º HTTP –∫–ª–∏–µ–Ω—Ç —Å –ø—Ä–æ–∫—Å–∏: {self.proxy_address}")
            return httpx.AsyncClient(
                proxy=self.proxy_address,
                timeout=httpx.Timeout(30.0, connect=10.0)
            )
        else:
            print("[LLM] –°–æ–∑–¥–∞–µ–º HTTP –∫–ª–∏–µ–Ω—Ç –±–µ–∑ –ø—Ä–æ–∫—Å–∏")
            return httpx.AsyncClient(
                timeout=httpx.Timeout(30.0, connect=10.0)
            )

    async def _make_request(self, messages: List[Dict[str, str]], temperature: float, max_completion_tokens: int) -> str:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ OpenAI API"""
        http_client = None
        openai_client = None
        
        try:
            # –°–æ–∑–¥–∞–µ–º HTTP –∫–ª–∏–µ–Ω—Ç
            http_client = await self._create_http_client()
            print("[LLM] HTTP –∫–ª–∏–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
            
            # –°–æ–∑–¥–∞–µ–º OpenAI –∫–ª–∏–µ–Ω—Ç
            openai_client = AsyncOpenAI(
                api_key=self.api_key,
                http_client=http_client,
                max_retries=2
            )
            print("[LLM] OpenAI –∫–ª–∏–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
            print(f"[LLM] –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏ {self.model}")
            response = await openai_client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_completion_tokens=max_completion_tokens,
                top_p=0.95,
                frequency_penalty=0.3,
            )
            
            choice = response.choices[0]
            finish_reason = choice.finish_reason
            
            if finish_reason == "length":
                print(f"[LLM] –í–ù–ò–ú–ê–ù–ò–ï: –û—Ç–≤–µ—Ç –æ–±—Ä–µ–∑–∞–Ω –∏–∑-–∑–∞ –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤ (max_completion_tokens={max_completion_tokens})")
            
            content = choice.message.content or ""
            print(f"[LLM] –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –¥–ª–∏–Ω–æ–π {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
            return content
            
        except PermissionDeniedError as e:
            print(f"[LLM] Permission denied: {e}", file=sys.stderr)
            if self.use_proxy:
                return "–æ–π, –ø—Ä–æ–±–ª–µ–º—ã —Å –ø—Ä–æ–∫—Å–∏... –ø—Ä–æ–≤–µ—Ä—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"
            else:
                return "–¥–æ—Å—Ç—É–ø –æ–≥—Ä–∞–Ω–∏—á–µ–Ω... –º–æ–∂–µ—Ç, –Ω—É–∂–µ–Ω –ø—Ä–æ–∫—Å–∏?"
                
        except AuthenticationError as e:
            print(f"[LLM] Authentication error: {e}", file=sys.stderr)
            return "–æ–π, –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–ª—é—á–æ–º API... –ø—Ä–æ–≤–µ—Ä—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"
            
        except APITimeoutError as e:
            print(f"[LLM] Timeout error: {e}", file=sys.stderr)
            return "—Ö–º, —á—Ç–æ-—Ç–æ –¥–æ–ª–≥–æ –¥—É–º–∞—é... –º–æ–∂–µ—Ç, —Å–ø—Ä–æ—Å–∏—à—å –ø–æ–ø—Ä–æ—â–µ?"
            
        except Exception as e:
            print(f"[LLM] Unexpected error: {e}", file=sys.stderr)
            print(f"[LLM] Error type: {type(e).__name__}", file=sys.stderr)
            traceback.print_exc()
            return "–æ–π, —á—Ç–æ-—Ç–æ —Å–≤—è–∑—å –±–∞—Ä–∞—Ö–ª–∏—Ç... –ø–æ–ø—Ä–æ–±—É–π –µ—â—ë —Ä–∞–∑?"
            
        finally:
            # –ó–∞–∫—Ä—ã–≤–∞–µ–º –∫–ª–∏–µ–Ω—Ç—ã
            if openai_client:
                try:
                    await openai_client.close()
                    print("[LLM] OpenAI –∫–ª–∏–µ–Ω—Ç –∑–∞–∫—Ä—ã—Ç")
                except Exception as e:
                    print(f"[LLM] –û—à–∏–±–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è OpenAI –∫–ª–∏–µ–Ω—Ç–∞: {e}")
            
            if http_client:
                try:
                    await http_client.aclose()
                    print("[LLM] HTTP –∫–ª–∏–µ–Ω—Ç –∑–∞–∫—Ä—ã—Ç")
                except Exception as e:
                    print(f"[LLM] –û—à–∏–±–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è HTTP –∫–ª–∏–µ–Ω—Ç–∞: {e}")

    async def _shorten_response(self, original_text: str) -> str:
        """–°–æ–∫—Ä–∞—â–∞–µ—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç"""
        print(f"[LLM] –°–æ–∫—Ä–∞—â–∞–µ–º –¥–ª–∏–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç ({len(original_text)} —Å–∏–º–≤–æ–ª–æ–≤)")
        
        messages = [
            {"role": "system", "content": "–¢—ã –ê–ª–∏–Ω–∞. –°–æ–∫—Ä–∞—Ç–∏ —Å–≤–æ–π –æ—Ç–≤–µ—Ç, –æ—Å—Ç–∞–≤–∏–≤ —Ç–æ–ª—å–∫–æ —Å–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ –∏ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–µ. –ú–∞–∫—Å–∏–º—É–º 5 –ø—É–Ω–∫—Ç–æ–≤ –¥–ª—è —Å–ø–∏—Å–∫–æ–≤. –°–æ—Ö—Ä–∞–Ω–∏ —Ç—ë–ø–ª—ã–π —Ç–æ–Ω."},
            {"role": "assistant", "content": original_text},
            {"role": "user", "content": "–≠—Ç–æ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ. –°–æ–∫—Ä–∞—Ç–∏ –¥–æ —Å–∞–º–æ–≥–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–≥–æ, –æ—Å—Ç–∞–≤—å –º–∞–∫—Å–∏–º—É–º 5 –ø—É–Ω–∫—Ç–æ–≤ –µ—Å–ª–∏ —ç—Ç–æ —Å–ø–∏—Å–æ–∫."}
        ]
        
        try:
            shortened = await self._make_request(messages, temperature=0.7, max_completion_tokens=400)
            print(f"[LLM] –û—Ç–≤–µ—Ç —Å–æ–∫—Ä–∞—â—ë–Ω –¥–æ {len(shortened)} —Å–∏–º–≤–æ–ª–æ–≤")
            return shortened
        except Exception as e:
            print(f"[LLM] –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–∏: {e}")
            # –ú–µ—Ö–∞–Ω–∏—á–µ—Å–∫–æ–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ
            lines = original_text.split('\n')
            if len(lines) > 7:
                result = '\n'.join(lines[:5])
                result += "\n\n—Ö–æ—á–µ—à—å –µ—â—ë? –º–æ–≥—É —Ä–∞—Å—Å–∫–∞–∑–∞—Ç—å –±–æ–ª—å—à–µ üíõ"
                return result
            return original_text[:MAX_RESPONSE_LENGTH] + "..."

    async def chat(
        self,
        messages: List[Dict[str, str]],
        temperature: Optional[float] = None,
        max_completion_tokens: Optional[int] = None,
        *,
        verbosity: Optional[str] = None,
        safety: bool = False,
    ) -> str:
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ OpenAI API"""
        
        temperature = float(temperature if temperature is not None else DEFAULT_TEMPERATURE)
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π max_completion_tokens
        if verbosity == "short":
            max_completion_tokens = SHORT_RESPONSE_LENGTH
        elif verbosity == "long":
            max_completion_tokens = MAX_RESPONSE_LENGTH  
        else:
            max_completion_tokens = int(max_completion_tokens if max_completion_tokens is not None else DEFAULT_max_completion_tokens)

        if safety:
            messages = [{"role": "system", "content": REFUSAL_STYLE}] + messages

        # –î–æ–±–∞–≤–ª—è–µ–º —É–∫–∞–∑–∞–Ω–∏—è –ø—Ä–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–æ–≤
        last_user_msg = messages[-1].get("content", "").lower()
        if any(word in last_user_msg for word in ["—Ñ–∞–∫—Ç", "–ø—É–Ω–∫—Ç", "—Å–ø–∏—Å–æ–∫", "–ø—Ä–∏—á–∏–Ω", "—Å–ø–æ—Å–æ–±"]):
            messages.append({
                "role": "system", 
                "content": "–í–ê–ñ–ù–û: –î–∞–∂–µ –µ—Å–ª–∏ –ø—Ä–æ—Å—è—Ç –º–Ω–æ–≥–æ –ø—É–Ω–∫—Ç–æ–≤, –¥–∞–π –º–∞–∫—Å–∏–º—É–º 5 —Å–∞–º—ã—Ö –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã—Ö. –ö–∞–∂–¥—ã–π –ø—É–Ω–∫—Ç —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏. –í –∫–æ–Ω—Ü–µ –º–æ–∂–µ—à—å —Å–ø—Ä–æ—Å–∏—Ç—å, —Ö–æ—á–µ—Ç –ª–∏ —á–µ–ª–æ–≤–µ–∫ –µ—â—ë."
            })
        else:
            messages.append({
                "role": "system", 
                "content": "–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É. –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –ø–æ–ª—É—á–∞–µ—Ç—Å—è –¥–ª–∏–Ω–Ω—ã–º, —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á—å—Å—è –Ω–∞ –≥–ª–∞–≤–Ω–æ–º."
            })

        print(f"[LLM] –ó–∞–ø—Ä–æ—Å –∫ {self.model} —Å max_completion_tokens={max_completion_tokens}, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞={temperature}")
        
        try:
            txt = await self._make_request(messages, temperature, max_completion_tokens)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–∞
            if len(txt) > MAX_RESPONSE_LENGTH:
                txt = await self._shorten_response(txt)
            
            return _postprocess(txt)
            
        except Exception as e:
            print(f"[LLM] –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ chat(): {e}", file=sys.stderr)
            return "—á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫... –ø–æ–ø—Ä–æ–±—É–π –µ—â—ë —Ä–∞–∑?"

    async def aclose(self):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
        pass